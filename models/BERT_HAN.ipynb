{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-HAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGnsMN3aU7KX"
      },
      "source": [
        "# 0 - Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O7kaRJoN-fC",
        "outputId": "5380f696-0216-43ca-b502-d36906fad9a0"
      },
      "source": [
        "# import package to load BERT model\n",
        "!pip install transformers\n",
        "\n",
        "# mount google drive to load dataset \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# for data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import string\n",
        "\n",
        "# pytorch module for model implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For saving model\n",
        "from collections import OrderedDict\n",
        "import urllib.request\n",
        "import pickle"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkLdut3NXm06",
        "outputId": "8b3462e8-9a4e-4dbd-d5c2-d7c03e25b811"
      },
      "source": [
        "# Set up CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxl4TWmfVKwH"
      },
      "source": [
        "# 1 - Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "xfnvqOyQTFXF",
        "outputId": "566e2871-6d2d-44b5-cc36-30faf25625b5"
      },
      "source": [
        "# Load dataset\n",
        "train_songs = pd.read_csv('/content/drive/MyDrive/COMP89/train.csv')\n",
        "\n",
        "# filter to only English songs \n",
        "train_songs = train_songs.loc[train_songs.Language == 'en']\n",
        "\n",
        "# convert to categorical to get numerical classes\n",
        "train_songs['Genre'] = train_songs['Genre'].astype('category') \n",
        "\n",
        "\n",
        "val_counts = train_songs['Genre'].value_counts()\n",
        "\n",
        "new_songs = train_songs.loc[train_songs.Genre == val_counts.index[0]].sample(n=min(train_songs['Genre'].value_counts()), random_state=42)\n",
        "for g in val_counts.index[1:]:\n",
        "  genre_df = train_songs.loc[train_songs.Genre == g]\n",
        "  genre_df = genre_df.sample(n=min(train_songs['Genre'].value_counts()), random_state=42)\n",
        "\n",
        "  new_songs = pd.concat([new_songs, genre_df])\n",
        "\n",
        "new_songs.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Artist</th>\n",
              "      <th>Song</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Language</th>\n",
              "      <th>Lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>247351</th>\n",
              "      <td>manic street preachers</td>\n",
              "      <td>motorcycle emptiness</td>\n",
              "      <td>Rock</td>\n",
              "      <td>en</td>\n",
              "      <td>Culture sucks down words\\nItemize loathing and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50981</th>\n",
              "      <td>santana</td>\n",
              "      <td>put your lights on</td>\n",
              "      <td>Rock</td>\n",
              "      <td>en</td>\n",
              "      <td>Hey now, all you sinners\\nPut your lights on, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35632</th>\n",
              "      <td>matchbox 20</td>\n",
              "      <td>bent</td>\n",
              "      <td>Rock</td>\n",
              "      <td>en</td>\n",
              "      <td>If I fall along the way\\nPick me up and dust m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273875</th>\n",
              "      <td>count the stars</td>\n",
              "      <td>all good things</td>\n",
              "      <td>Rock</td>\n",
              "      <td>en</td>\n",
              "      <td>This air is contagious, no one can save us, no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29567</th>\n",
              "      <td>killing joke</td>\n",
              "      <td>democracy</td>\n",
              "      <td>Rock</td>\n",
              "      <td>en</td>\n",
              "      <td>You have a choice, we are your voice\\nRed, blu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Artist  ...                                             Lyrics\n",
              "247351  manic street preachers  ...  Culture sucks down words\\nItemize loathing and...\n",
              "50981                  santana  ...  Hey now, all you sinners\\nPut your lights on, ...\n",
              "35632              matchbox 20  ...  If I fall along the way\\nPick me up and dust m...\n",
              "273875         count the stars  ...  This air is contagious, no one can save us, no...\n",
              "29567             killing joke  ...  You have a choice, we are your voice\\nRed, blu...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY6tZT11WeYZ"
      },
      "source": [
        "# 2 - Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6RrgNSXxZp"
      },
      "source": [
        "## 2.1 - Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGSsFB-CTKAw"
      },
      "source": [
        "# bert word attention\n",
        "class Word_RNN(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Word_RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = False).to(device)\n",
        "\n",
        "        self.word_weight = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.word_context_weight = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def forward(self, raw_sents):\n",
        "        sents = torch.zeros(1,len(raw_sents),self.hidden_size).to(device)\n",
        "\n",
        "        encoded_input = self.tokenizer(raw_sents, return_tensors='pt', padding = True, truncation = True).to(device)\n",
        "        h = self.bert_model(**encoded_input)[0]\n",
        "        for i in range(len(raw_sents)):\n",
        "            h_i = h[i, :, :].unsqueeze(0)\n",
        "            u_i = torch.tanh(self.word_weight(h_i))\n",
        "            u_iTw = self.word_context_weight(u_i).squeeze(2)\n",
        "\n",
        "            attn_weights = F.softmax(u_iTw, dim=1)\n",
        "            s_i = (attn_weights * h_i.permute(0,2,1)).sum(dim = 2)\n",
        "            sents[:,i,:] = s_i\n",
        "        return sents\n",
        "\n",
        "# sentence attention\n",
        "class Sent_RNN(nn.Module):\n",
        "    def __init__(self, word_num_hidden, sentence_num_hidden):\n",
        "        super(Sent_RNN, self).__init__()\n",
        "        self.sentence_num_hidden = sentence_num_hidden\n",
        "\n",
        "        self.lstm = nn.LSTM(word_num_hidden, sentence_num_hidden, bidirectional=True, batch_first = True)\n",
        "\n",
        "        self.sent_weight = nn.Linear(2*sentence_num_hidden, 2*sentence_num_hidden)\n",
        "        self.sent_context_weight = nn.Linear(2*sentence_num_hidden, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_is, _ = self.lstm(x)\n",
        "        u_is = torch.tanh(self.sent_weight(h_is))\n",
        "        u_iTs = self.sent_context_weight(u_is).squeeze(2)\n",
        "        a_is = F.softmax(u_iTs, dim=1)\n",
        "\n",
        "        v = (a_is * h_is.permute(0,2,1)).sum(dim = 2)\n",
        "        return v\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, sentence_num_hidden, word_hidden_size, num_classes):\n",
        "        super(HAN, self).__init__()\n",
        "\n",
        "        self.word_attn_rnn = Word_RNN(word_hidden_size)\n",
        "        self.sent_attn_rnn = Sent_RNN(word_hidden_size, sentence_num_hidden)\n",
        "\n",
        "        self.linear = nn.Linear(2*sentence_num_hidden, num_classes)\n",
        "    \n",
        "    def forward(self, raw_sents):\n",
        "        word_embeddings = self.word_attn_rnn(raw_sents)\n",
        "        sent_embeddings = self.sent_attn_rnn(word_embeddings)\n",
        "\n",
        "        scores = self.linear(sent_embeddings)\n",
        "\n",
        "        return scores"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOKf7cOoWutT",
        "outputId": "bf301cd6-b976-48f8-d344-a9ece7f85016"
      },
      "source": [
        "# Define model hyperparamters\n",
        "sentence_num_hidden = 256\n",
        "word_hidden_size = 768\n",
        "num_classes = new_songs['Genre'].nunique()\n",
        "\n",
        "# Create instance of model\n",
        "model = HAN(sentence_num_hidden, word_hidden_size, num_classes).to(device)\n",
        "\n",
        "# Freeze all BERT layers from training\n",
        "non_bert_params = []\n",
        "for name, _param in model.named_parameters():\n",
        "    if 'bert' not in name:\n",
        "        non_bert_params.append(_param)\n",
        "    else:\n",
        "        _param.requires_grad = False\n",
        "\n",
        "# Define training paramters\n",
        "num_epochs = 5\n",
        "lr = 0.01\n",
        "\n",
        "# Define loss function and optimiser\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.SGD(non_bert_params, lr=lr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4XPfrAGX3Gb"
      },
      "source": [
        "## 2.2 - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQKailGhttu"
      },
      "source": [
        "# Helper functions for accuracy\n",
        "def predict(X):\n",
        "    preds = []\n",
        "    model.eval()\n",
        "    for x in X:\n",
        "        bars = x.split('/n')\n",
        "        out = model(bars)\n",
        "        train_preds.append(torch.argmax(F.softmax(out)).item())\n",
        "    \n",
        "    return preds\n",
        "\n",
        "def Accuracy(preds, label):\n",
        "    return np.mean(np.array(preds) == np.array(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpv-jDGgixto"
      },
      "source": [
        "<font color='red'>DO NOT RUN THIS CELL (WILL TAKE > 5 HOURS)</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4To71dilXUbe",
        "outputId": "a866dee2-e96c-4473-b6ca-bf628c045e08"
      },
      "source": [
        "X = new_songs.Lyrics.values.copy()\n",
        "y = new_songs.Genre.cat.codes.values\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    # shuffle dataset for each epoch\n",
        "    X, y = shuffle(X, y)\n",
        "\n",
        "    # SGD\n",
        "    for x, label in zip (X, y):\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # split raw text input by each line in song\n",
        "        bars = x.split('/n')\n",
        "        out = model(bars)\n",
        "\n",
        "        label = torch.LongTensor([label]).to(device)\n",
        "\n",
        "        # find the loss\n",
        "        train_loss = criterion(out, label)\n",
        "\n",
        "        #backprop\n",
        "        train_loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "# Training Accuracy\n",
        "print(\"Train acc: {}\".format(Accuracy(predict(X), y)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train acc: 0.44253463894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCFR0cFdPN7"
      },
      "source": [
        "## 2.3 - Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVjCbOyDYFmd"
      },
      "source": [
        "partial_state_dict = OrderedDict()\n",
        "\n",
        "for param_name in list(model.state_dict().keys()):\n",
        "    if 'bert' not in param_name:\n",
        "        partial_state_dict[param_name] = model.state_dict()[param_name]\n",
        "\n",
        "# export\n",
        "torch.save(partial_state_dict, '/content/drive/MyDrive/COMP89/partial_model_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXzVGN36d58a"
      },
      "source": [
        "# 3 - Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70WjHvq_d7Br"
      },
      "source": [
        "# Import model weights\n",
        "url = 'https://github.com/salkhalil/Lyrics2Vec/raw/main/saved_embeddings/partial_model_weights'\n",
        "urllib.request.urlretrieve(url, './partial_model')\n",
        "\n",
        "# Load Model\n",
        "model = HAN(sentence_num_hidden, word_hidden_size, num_classes).to(device)\n",
        "model.load_state_dict(torch.load('./partial_model'), strict=False)\n",
        "\n",
        "# Import test set\n",
        "test_data = pd.read_csv('https://github.com/salkhalil/Lyrics2Vec/raw/main/datasets/cheeky.csv')\n",
        "\n",
        "X = test_data.Lyrics.values.copy()\n",
        "y = test_data.Genre.cat.codes.values\n",
        "\n",
        "print(\"Test acc: {}\".format(Accuracy(predict(X_test), y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9JR8ptKeJYk"
      },
      "source": [
        "# loss plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS4k3sQ5eJ-j"
      },
      "source": [
        "# confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}