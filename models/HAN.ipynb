{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../saved_embeddings/enc_bar', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>In Da Club</td>\n",
       "      <td>[Intro]\\nGo, go, go, go, go, go\\nGo Shorty, it...</td>\n",
       "      <td>50 Cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21 Questions</td>\n",
       "      <td>[Ad-Libs]\\nNew York City\\nYou are now rockin'\\...</td>\n",
       "      <td>50 Cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Many Men (Wish Death)</td>\n",
       "      <td>[Skit]\\nMan, we gotta go get somethin' to eat\\...</td>\n",
       "      <td>50 Cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>My Life</td>\n",
       "      <td>[Chorus]\\nMy, yeah, yeah, mmm\\nMy life, my lif...</td>\n",
       "      <td>50 Cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Patiently Waiting</td>\n",
       "      <td>[Intro]\\nHey Em, you know you're my favorite w...</td>\n",
       "      <td>50 Cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>Cruisin’</td>\n",
       "      <td>[Intro]\\n(Crusin')\\n\\n[Verse 1]\\nBaby, let's c...</td>\n",
       "      <td>Smokey Robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>Really Gonna Miss You</td>\n",
       "      <td>[Verse 1]\\nReally gonna miss you\\nIt's really ...</td>\n",
       "      <td>Smokey Robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>The Agony and the Ecstasy</td>\n",
       "      <td>[Verse 1]\\nWhat's it all about this crazy love...</td>\n",
       "      <td>Smokey Robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>Ooh Baby Baby</td>\n",
       "      <td>[Verse 1]\\nI did you wrong\\nMy heart went out ...</td>\n",
       "      <td>Smokey Robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>My Girl</td>\n",
       "      <td>[Intro]\\nOoh, ooh-ooh, ooh (Ooh)\\nOoh, ooh-ooh...</td>\n",
       "      <td>Smokey Robinson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    song title  \\\n",
       "0                   In Da Club   \n",
       "1                 21 Questions   \n",
       "2        Many Men (Wish Death)   \n",
       "3                      My Life   \n",
       "4            Patiently Waiting   \n",
       "..                         ...   \n",
       "575                   Cruisin’   \n",
       "576      Really Gonna Miss You   \n",
       "579  The Agony and the Ecstasy   \n",
       "582              Ooh Baby Baby   \n",
       "587                    My Girl   \n",
       "\n",
       "                                                lyrics           artist  \n",
       "0    [Intro]\\nGo, go, go, go, go, go\\nGo Shorty, it...          50 Cent  \n",
       "1    [Ad-Libs]\\nNew York City\\nYou are now rockin'\\...          50 Cent  \n",
       "2    [Skit]\\nMan, we gotta go get somethin' to eat\\...          50 Cent  \n",
       "3    [Chorus]\\nMy, yeah, yeah, mmm\\nMy life, my lif...          50 Cent  \n",
       "4    [Intro]\\nHey Em, you know you're my favorite w...          50 Cent  \n",
       "..                                                 ...              ...  \n",
       "575  [Intro]\\n(Crusin')\\n\\n[Verse 1]\\nBaby, let's c...  Smokey Robinson  \n",
       "576  [Verse 1]\\nReally gonna miss you\\nIt's really ...  Smokey Robinson  \n",
       "579  [Verse 1]\\nWhat's it all about this crazy love...  Smokey Robinson  \n",
       "582  [Verse 1]\\nI did you wrong\\nMy heart went out ...  Smokey Robinson  \n",
       "587  [Intro]\\nOoh, ooh-ooh, ooh (Ooh)\\nOoh, ooh-ooh...  Smokey Robinson  \n",
       "\n",
       "[488 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_songs = pd.read_csv('../datasets/all_songs.csv')\n",
    "all_songs['artist'] = all_songs['artist'].astype('category') # convert to categorical to get numerical classes\n",
    "some_songs = all_songs.loc[all_songs['lyrics'].str.startswith('[').fillna(False)]\n",
    "some_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "y = some_songs.artist.cat.codes.values\n",
    "\n",
    "# del idx 158 & 224\n",
    "y = np.delete(y, [158,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.9, shuffle = True)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Word Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLevelRNN(nn.Module):\n",
    "    def __init__(self, word_num_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_context_weights = nn.Parameter(torch.rand(2 * word_num_hidden, 1))\n",
    "        self.word_context_weights.data.uniform_(-0.25, 0.25)\n",
    "        \n",
    "        self.GRU = nn.GRU(word_num_hidden, word_num_hidden, 1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2 * word_num_hidden, 2 * word_num_hidden, bias=True)\n",
    "        \n",
    "        self.soft_word = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN layer\n",
    "        h, _ = self.GRU(x)\n",
    "        x = torch.tanh(self.linear(h))\n",
    "        \n",
    "        # attention layer\n",
    "        x = torch.matmul(x, self.word_context_weights)\n",
    "        x = x.squeeze(dim=2)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.soft_word(x.transpose(1, 0))\n",
    "        x = torch.mul(h.permute(2, 0, 1), x.transpose(1, 0))\n",
    "        x = torch.sum(x, dim=2).transpose(1, 0).unsqueeze(0)\n",
    "        \n",
    "        # should output bars combined into one verse embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Sentence Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentLevelRNN(nn.Module):\n",
    "    def __init__(self, sentence_num_hidden, word_num_hidden, target_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sentence_context_weights = nn.Parameter(torch.rand(2 * sentence_num_hidden, 1))\n",
    "        self.sentence_context_weights.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        self.sentence_gru = nn.GRU(2 * word_num_hidden, sentence_num_hidden, bidirectional=True)\n",
    "        self.sentence_linear = nn.Linear(2 * sentence_num_hidden, 2 * sentence_num_hidden, bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * sentence_num_hidden , target_class)\n",
    "        self.soft_sent = nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # RNN layer\n",
    "        sentence_h,_ = self.sentence_gru(x)\n",
    "        x = torch.tanh(self.sentence_linear(sentence_h))\n",
    "        \n",
    "        # attention layer\n",
    "        x = torch.matmul(x, self.sentence_context_weights)\n",
    "        x = x.squeeze(dim=2)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.soft_sent(x.transpose(1,0))\n",
    "        x = torch.mul(sentence_h.permute(2, 0, 1), x.transpose(1, 0))\n",
    "        x = torch.sum(x, dim=1).transpose(1, 0).unsqueeze(0)\n",
    "        x = self.fc(x.squeeze(0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "\n",
    "    def __init__(self, word_num_hidden, sentence_num_hidden, target_class):\n",
    "        super().__init__()\n",
    "        self.word_attention_rnn = WordLevelRNN(word_num_hidden)\n",
    "        self.sentence_attention_rnn = SentLevelRNN(sentence_num_hidden, word_num_hidden, target_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # todo include the fact that the embeddings are already passed in\n",
    "        \n",
    "        # x is list of tensors of size [1: #bar: #embedding size]\n",
    "#         x = x.permute(1, 2, 0) # Expected : # sentences, # words, batch size\n",
    "\n",
    "        num_verses = len(x)\n",
    "        word_attentions = None\n",
    "        \n",
    "        for i in range(num_verses):\n",
    "            # x[i] is a verse of size (1, #bars, 768)\n",
    "            if len(x[i].size()) != 3:\n",
    "                continue\n",
    "            word_attn = self.word_attention_rnn(x[i])\n",
    "            if word_attentions is None:\n",
    "                word_attentions = word_attn\n",
    "            else:\n",
    "                word_attentions = torch.cat((word_attentions, word_attn), 0)\n",
    "        return self.sentence_attention_rnn(word_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "word_num_hidden = 768\n",
    "words_dim = 768\n",
    "sentence_num_hidden = 128\n",
    "target_class = 13\n",
    "\n",
    "model = HAN(word_num_hidden, sentence_num_hidden, target_class)\n",
    "\n",
    "# define model parameters\n",
    "num_epochs = 1\n",
    "lr = 0.05\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiru/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/hiru/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5858559608459473\n",
      "14.28847599029541\n",
      "14.225384712219238\n",
      "14.215431213378906\n",
      "15.487299919128418\n",
      "17.07025146484375\n",
      "17.10393714904785\n",
      "4.350581169128418\n",
      "19.57310676574707\n",
      "15.474870681762695\n",
      "6.126579284667969\n",
      "14.080460548400879\n",
      "12.325483322143555\n",
      "5.264707088470459\n",
      "20.008594512939453\n",
      "3.0332651138305664\n",
      "1.422182559967041\n",
      "1.7472666501998901\n",
      "12.639689445495605\n",
      "7.114170551300049\n",
      "12.562002182006836\n",
      "24.915555953979492\n",
      "5.526189804077148\n",
      "5.6098103523254395\n",
      "2.287616014480591\n",
      "16.94512939453125\n",
      "5.457031726837158\n",
      "5.068414688110352\n",
      "5.481394290924072\n",
      "13.030878067016602\n",
      "11.7523832321167\n",
      "0.9477710127830505\n",
      "12.075323104858398\n",
      "11.150036811828613\n",
      "1.6303582191467285\n",
      "2.648916244506836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-1eed3cdc182a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    all_losses = []\n",
    "    for x_val, y_val in zip(x_train, y_train):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # x_val has length = # of verses\n",
    "        out = model(x_val)\n",
    "        \n",
    "        train_loss = criterion(out, torch.tensor([y_val]))\n",
    "        avg_loss.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        optimiser.step()\n",
    "    avg_loss = sum(avg_loss)/len(avg_loss)\n",
    "    avg_losses.append(avg_loss)\n",
    "    print('Epoch: {} .......... Training Loss: {}'.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(xs, ys):\n",
    "    correct = 0\n",
    "    for i in range(len(xs)):\n",
    "        # run through model\n",
    "        test_sample = xs[i].to(device)\n",
    "        pred = model(test_sample)\n",
    "        # calc argmax\n",
    "        pred = torch.argmax(pred).item()\n",
    "        # sum up correct predictions\n",
    "        correct += (pred == ys[i].item())\n",
    "    return correct/len(xs)\n",
    "\n",
    "test_acc = Accuracy(x_test, y_test)\n",
    "print('{}% Test Accuracy'.format(test_acc*100))\n",
    "\n",
    "train_acc = Accuracy(x_train, y_train)\n",
    "print('{}% Train Accuracy'.format(train_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each song should be a list of verses\n",
    "Where each verse is a matrix of (1, #words, BERT_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
